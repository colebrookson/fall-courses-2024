---
title: BIS 567 HW05
format: 
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
authors:
  - name: Cole Brookson
---

## Question 1

### Part (A)

```{r}
library(knitr)
library(kableExtra)
library(magrittr)
load(here::here("./bis567/homework/hw05/HW5.RData"))
set.seed(123)

# values that we need for matrix size etc
n <- length(y)
p <- ncol(x)
n_iter <- 100000 # num of iterations
burn_in <- 2000 # burn in
thin_interval <- 10 # thinning interval

# function to calculate the log posterior
log_posterior <- function(beta, y, x) {
    eta <- x %*% beta
    log_lik <- sum(y * eta - log(1 + exp(eta)))
    log_prior <- -0.5 * t(beta) %*% beta / 10000
    return(log_lik + log_prior)
}

# initialize the important bits
beta_init <- rep(0, p)
proposal_sd <- 0.2 # 0.2 gets about 50% acceptance
beta_samples <- matrix(0, nrow = n_iter, ncol = p)
beta_samples[1, ] <- beta_init

accept_count <- 0 # init the acceptance counter
for (i in 2:n_iter) {
    current_beta <- beta_samples[i - 1, ]

    # new beta
    proposed_beta <- current_beta + rnorm(p, mean = 0, sd = proposal_sd)

    # this function is pre-written
    log_posterior_current <- log_posterior(current_beta, y, x)
    log_posterior_proposed <- log_posterior(proposed_beta, y, x)

    # acceptance prob
    accept_prob <- exp(log_posterior_proposed - log_posterior_current)

    # reject or
    if (runif(1) < accept_prob) {
        beta_samples[i, ] <- proposed_beta
        accept_count <- accept_count + 1 # inc
    } else {
        beta_samples[i, ] <- current_beta
    }

    # for testing
    if (i %% 1000 == 0) {
        acceptance_rate <- accept_count / i
        cat("Iteration", i, "Acceptance Rate:", round(acceptance_rate, 4), "\n")
    }
}

# now discard the burn-in
post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]
# thin the samples!
thinned_samples <- post_burn_samples[seq(1, nrow(post_burn_samples), by = thin_interval), ]

# provide the inference on ONLY the thinned parts
summary_stats_thinned <- apply(thinned_samples, 2, function(x) {
    c(Mean = mean(x), SD = sd(x), `2.5%` = quantile(x, 0.025), `97.5%` = quantile(x, 0.975))
})

summary_stats_df <- as.data.frame(t(summary_stats_thinned))
summary_stats_df$parameter <- c("beta1", "beta2")
summary_stats_df_metrop <- summary_stats_df %>%
    dplyr::select(parameter, names(.)[1:4])

summary_table <- summary_stats_df_metrop %>%
    knitr::kable("html",
        caption = "Summary Statistics for Thinned Samples for Question 1a)",
        col.names = c("Parameter", "Estimate", "SD", "2.5%", "97.5%")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
print(summary_table)

# diagnostics!
geweke_results <- coda::geweke.diag(thinned_samples)
ess_results <- coda::effectiveSize(thinned_samples)

diags_df_metrop <- data.frame(
    parameter = c("beta1", "beta2"),
    ESS = unname(ess_results),
    geweke = unname(geweke_results$z)
)
diags_table <- diags_df_metrop %>%
    knitr::kable("html",
        caption = "Convergence Diagnostics for Thinned Samples for Question 1a)",
        col.names = c("Parameter", "Effective Sample Size", "Geweke Diagnostic")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
```
```{r}
# plotting
df_metropolis <- dplyr::as_tibble(data.frame(thinned_samples)) %>%
    dplyr::rename(beta1 = X1, beta2 = X2) %>%
    dplyr::mutate(
        iteration = seq(1, nrow(post_burn_samples), by = thin_interval),
        method = "Metropolis"
    )
df_metropolis_long <- dplyr::as_tibble(df_metropolis %>%
    tidyr::pivot_longer(
        cols = c("beta1", "beta2"),
        names_to = "parameter",
        values_to = "value"
    ))
trace_plot_metropolis <- ggplot2::ggplot(
    df_metropolis_long,
    ggplot2::aes(x = iteration, y = value, colour = parameter)
) +
    ggplot2::geom_line() +
    ggplot2::facet_wrap(~parameter, scales = "free_y") +
    ggplot2::labs(title = "Trace Plots for Metropolis Algorithm", y = "Parameter Value", x = "Iteration") +
    ggplot2::theme_bw()

acf_metropolis_beta1 <- acf(thinned_samples[, 1], plot = FALSE)
acf_metropolis_beta2 <- acf(thinned_samples[, 2], plot = FALSE)
acf_df_metropolis_beta1 <- dplyr::as_tibble(data.frame(
    lag = acf_metropolis_beta1$lag,
    acf = acf_metropolis_beta1$acf, parameter = "beta1"
))
acf_df_metropolis_beta2 <- dplyr::as_tibble(data.frame(
    lag = acf_metropolis_beta2$lag,
    acf = acf_metropolis_beta2$acf, parameter = "beta2"
))
acf_df_metropolis <- dplyr::bind_rows(
    acf_df_metropolis_beta1,
    acf_df_metropolis_beta2
) %>%
    dplyr::mutate(method = "Metropolis")
acf_plot_metropolis <- ggplot2::ggplot(
    acf_df_metropolis, ggplot2::aes(
        x = lag, y = acf,
        colour = parameter, fill = parameter
    )
) +
    ggplot2::geom_bar(stat = "identity", width = 0.5) +
    ggplot2::facet_wrap(~parameter, scales = "free_y") +
    ggplot2::labs(title = "Autocorrelation Plots for Metropolis Algorithm", y = "ACF", x = "Lag") +
    ggplot2::theme_bw()
plot_metropolis <- patchwork::wrap_plots(
    trace_plot_metropolis,
    acf_plot_metropolis,
    ncol = 1
) +
    patchwork::plot_layout(ncol = 1)
```

### Part (B)
#### Part (i) 

* see handwritten paper *

#### Part (ii) 

* see handwritten paper * 

#### Part (iii)

```{r}
library(pgdraw)
library(mvtnorm)
n <- length(y)
p <- ncol(x)

n_iter <- 100000 # num of iterations
burn_in <- 2000 # burn in
thin_interval <- 10 # thinning interval

# init the values and objects to store things in
beta <- rep(0, p)
omega <- rep(1, n)
beta_samples <- matrix(0, nrow = n_iter, ncol = p)

for (t in 1:n_iter) {
    # current beta
    for (i in 1:n) {
        # this is the predicotr
        psi_i <- x[i, ] %*% beta

        # samp omega_i | beta using Polya-Gamma(1, psi_i)
        omega[i] <- pgdraw(1, psi_i)
    }

    # posterior covariance ?
    Omega <- diag(omega)
    Sigma_beta <- solve(t(x) %*% Omega %*% x + diag(1 / 10000, p))
    mu_beta <- Sigma_beta %*% t(x) %*% (y - 0.5)

    # draw beta
    beta <- as.vector(rmvnorm(1, mean = mu_beta, sigma = Sigma_beta))
    beta_samples[t, ] <- beta
}

# discard burnin
post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]

# thin them samples
thin_interval <- 10
thinned_samples <- post_burn_samples[seq(1, nrow(post_burn_samples), thin_interval), ]
summary_stats_thinned <- apply(thinned_samples, 2, function(x) {
    c(Mean = mean(x), SD = sd(x), `2.5%` = quantile(x, 0.025), `97.5%` = quantile(x, 0.975))
})

summary_stats_df <- as.data.frame(t(summary_stats_thinned))
summary_stats_df$parameter <- c("beta1", "beta2")
summary_stats_df_gibbs <- summary_stats_df %>%
    dplyr::select(parameter, names(.)[1:4])

# nice table
summary_table <- summary_stats_df_gibbs %>%
    knitr::kable("html",
        caption = "Summary Statistics for Thinned Samples for Question 1b-iii)",
        col.names = c("Parameter", "Estimate", "SD", "2.5%", "97.5%")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
print(summary_table)

# diagnostics!
geweke_results <- coda::geweke.diag(thinned_samples)
ess_results <- coda::effectiveSize(thinned_samples)

diags_df_gibbs <- data.frame(
    parameter = c("beta1", "beta2"),
    ESS = unname(ess_results),
    geweke = unname(geweke_results$z)
)
diags_table <- diags_df_gibbs %>%
    knitr::kable("html",
        caption = "Convergence Diagnostics for Thinned Samples for
        Question 11b-iii)",
        col.names = c("Parameter", "Effective Sample Size", "Geweke Diagnostic")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
```

```{r}
df_gibbs <- dplyr::as_tibble(data.frame(thinned_samples)) %>%
    dplyr::rename(beta1 = X1, beta2 = X2) %>%
    dplyr::mutate(
        iteration = seq(1, nrow(post_burn_samples), by = thin_interval),
        method = "Gibbs"
    )
df_gibbs_long <- dplyr::as_tibble(df_gibbs %>%
    tidyr::pivot_longer(
        cols = c("beta1", "beta2"),
        names_to = "parameter",
        values_to = "value"
    ))
trace_plot_gibbs <- ggplot2::ggplot(
    df_gibbs_long,
    ggplot2::aes(x = iteration, y = value, colour = parameter)
) +
    ggplot2::geom_line() +
    ggplot2::facet_wrap(~parameter, scales = "free_y") +
    ggplot2::labs(
        title = "Trace Plots for Gibbs Algorithm",
        y = "Parameter Value", x = "Iteration"
    ) +
    ggplot2::theme_bw()

acf_gibbs_beta1 <- acf(thinned_samples[, 1], plot = FALSE)
acf_gibbs_beta2 <- acf(thinned_samples[, 2], plot = FALSE)
acf_df_gibbs_beta1 <- dplyr::as_tibble(data.frame(
    lag = acf_gibbs_beta1$lag,
    acf = acf_gibbs_beta1$acf, parameter = "beta1"
))
acf_df_gibbs_beta2 <- dplyr::as_tibble(data.frame(
    lag = acf_gibbs_beta2$lag,
    acf = acf_gibbs_beta2$acf, parameter = "beta2"
))
acf_df_gibbs <- dplyr::bind_rows(
    acf_df_gibbs_beta1,
    acf_df_gibbs_beta2
) %>%
    dplyr::mutate(method = "gibbs")
acf_plot_gibbs <- ggplot2::ggplot(
    acf_df_gibbs, ggplot2::aes(
        x = lag, y = acf,
        colour = parameter, fill = parameter
    )
) +
    ggplot2::geom_bar(stat = "identity", width = 0.5) +
    ggplot2::facet_wrap(~parameter, scales = "free_y") +
    ggplot2::labs(
        title = "Autocorrelation Plots for Gibbs Algorithm",
        y = "ACF", x = "Lag"
    ) +
    ggplot2::theme_bw()
plot_gibbs <- patchwork::wrap_plots(
    trace_plot_gibbs,
    acf_plot_gibbs,
    ncol = 1
) +
    patchwork::plot_layout(ncol = 1)
```

## Question 1 
### Part (C)

```{r}
metropolis_sampling <- function(y, x) {
    n <- length(y)
    p <- ncol(x)
    n_iter <- 100000

    beta_init <- rep(0, p)
    proposal_sd <- 0.2 # 0.2 gets about 50% acceptance
    beta_samples <- matrix(0, nrow = n_iter, ncol = p)
    beta_samples[1, ] <- beta_init

    for (i in 2:n_iter) {
        current_beta <- beta_samples[i - 1, ]

        # Propose a new beta
        proposed_beta <- current_beta + rnorm(p, mean = 0, sd = proposal_sd)

        # Calculate log-posterior for current and proposed values
        log_posterior_current <- log_posterior(current_beta, y, x)
        log_posterior_proposed <- log_posterior(proposed_beta, y, x)

        # Acceptance probability
        accept_prob <- exp(log_posterior_proposed - log_posterior_current)

        # Accept or reject
        if (runif(1) < accept_prob) {
            beta_samples[i, ] <- proposed_beta
            accept_count <- accept_count + 1 # Increment if accepted
        } else {
            beta_samples[i, ] <- current_beta
        }
    }
}

gibbs_sampling <- function(y, x) {
    n <- length(y)
    p <- ncol(x)

    n_iter <- 100000 # num of iterations
    burn_in <- 2000 # burn in
    thin_interval <- 10 # thinning interval

    # init the values and objects to store things in
    beta <- rep(0, p)
    omega <- rep(1, n)
    beta_samples <- matrix(0, nrow = n_iter, ncol = p)

    for (t in 1:n_iter) {
        # current beta
        for (i in 1:n) {
            # this is the predicotr
            psi_i <- x[i, ] %*% beta

            # samp omega_i | beta using Polya-Gamma(1, psi_i)
            omega[i] <- pgdraw(1, psi_i)
        }

        # posterior covariance ?
        Omega <- diag(omega)
        Sigma_beta <- solve(t(x) %*% Omega %*% x + diag(1 / 10000, p))
        mu_beta <- Sigma_beta %*% t(x) %*% (y - 0.5)

        # draw beta
        beta <- as.vector(rmvnorm(1, mean = mu_beta, sigma = Sigma_beta))
        beta_samples[t, ] <- beta
    }
}
timing_results <- microbenchmark::microbenchmark(
    Metropolis_Hastings = metropolis_sampling(y, x),
    Gibbs_Sampling = gibbs_sampling(y, x),
    times = 1
)

time_metropolis <- median(timing_results$time[timing_results$expr == "Metropolis_Hastings"]) / 1e9
time_gibbs <- median(timing_results$time[timing_results$expr == "Gibbs_Sampling"]) / 1e9

comparison_df <- data.frame(
    method = c(rep("Metropolis", 2), rep("Gibbs", 2)),
    parameter = c("beta1", "beta2", "beta1", "beta2"),
    estimates = c(
        round(summary_stats_df_metrop$Mean, 3),
        round(summary_stats_df_gibbs$Mean, 3)
    ),
    low_ci = c(
        round(summary_stats_df_metrop$`2.5%.2.5%`, 3),
        round(summary_stats_df_gibbs$`2.5%.2.5%`, 3)
    ),
    hi_ci = c(
        round(summary_stats_df_metrop$`97.5%.97.5%`, 3),
        round(summary_stats_df_gibbs$`97.5%.97.5%`, 3)
    ),
    times = c(
        rep(round(time_metropolis, 2), 2),
        rep(round(time_gibbs, 2), 2)
    ),
    gweke = c(
        round(daigs_df_metrop$geweke, 3),
        round(diags_df_gibbs$geweke, 3)
    ),
    ess = c(
        diags_df_metrop$ESS,
        diags_df_gibbs$ESS
    )
)

comp_table <- comparison_df %>%
    knitr::kable("html",
        caption = "Comparison of Diagnostics between Metropolis-Hastings and Gibbs Sampling",
        col.names = c(
            "Method", "Parameter", "Estimate", "2.5%", "97.5%",
            "Median Time (seconds)", "Geweke Statistic Z-score",
            "Effective Sample Size"
        )
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "center",
        bootstrap_options = c("striped", "hover", "condensed")
    ) %>%
    kableExtra::column_spec(1, bold = TRUE, color = "darkblue") %>%
    kableExtra::column_spec(2:3, background = "lightgray")
print(comp_table)
```


We can see that the comparison table shows us that the two algorithms produce relatively similar results. The Gibbs sampler does take significantly longer, but the autocorrelation (seen in the plots above) is significantly lower, and the ESS is understandably higher. Likely the longer period of time for the Gibbs sampler is due to the fact that the auxilary varible needs to be drawn and since it's from a non-standard distribution that almost certainly adds compute time. 