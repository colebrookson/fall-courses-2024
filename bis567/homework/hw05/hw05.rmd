---
title: BIS 567 HW05
format: 
  pdf:
    toc: false
    number-sections: true
    colorlinks: true
authors:
  - name: Cole Brookson
---

## Question 1

### Part (A)

```{r}
library(knitr)
library(kableExtra)
library(magrittr)
load(here::here("./bis567/homework/hw05HW5.RData"))
set.seed(123)

# values that we need for matrix size etc
n <- length(y)
p <- ncol(x)
n_iter <- 100000 # num of iterations
burn_in <- 2000 # burn in
thin_interval <- 10 # thinning interval

# function to calculate the log posterior
log_posterior <- function(beta, y, x) {
    eta <- x %*% beta
    log_lik <- sum(y * eta - log(1 + exp(eta)))
    log_prior <- -0.5 * t(beta) %*% beta / 10000
    return(log_lik + log_prior)
}

# initialize the important bits
beta_init <- rep(0, p)
proposal_sd <- 0.2 # 0.2 gets about 50% acceptance
beta_samples <- matrix(0, nrow = n_iter, ncol = p)
beta_samples[1, ] <- beta_init

accept_count <- 0 # init the acceptance counter
for (i in 2:n_iter) {
    current_beta <- beta_samples[i - 1, ]

    # Propose a new beta
    proposed_beta <- current_beta + rnorm(p, mean = 0, sd = proposal_sd)

    # Calculate log-posterior for current and proposed values
    log_posterior_current <- log_posterior(current_beta, y, x)
    log_posterior_proposed <- log_posterior(proposed_beta, y, x)

    # Acceptance probability
    accept_prob <- exp(log_posterior_proposed - log_posterior_current)

    # Accept or reject
    if (runif(1) < accept_prob) {
        beta_samples[i, ] <- proposed_beta
        accept_count <- accept_count + 1 # Increment if accepted
    } else {
        beta_samples[i, ] <- current_beta
    }

    # Print acceptance rate every 1000 iterations
    if (i %% 1000 == 0) {
        acceptance_rate <- accept_count / i
        cat("Iteration", i, "Acceptance Rate:", round(acceptance_rate, 4), "\n")
    }
}

# now discard the burn-in
post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]
# thin the samples!
thinned_samples <- post_burn_samples[seq(1, nrow(post_burn_samples), by = thin_interval), ]

# provide the inference on ONLY the thinned parts
summary_stats_thinned <- apply(thinned_samples, 2, function(x) {
    c(Mean = mean(x), SD = sd(x), `2.5%` = quantile(x, 0.025), `97.5%` = quantile(x, 0.975))
})

# Convert to data frame for easier manipulation
summary_stats_df <- as.data.frame(t(summary_stats_thinned))
summary_stats_df$parameter <- c("beta1", "beta2")
summary_stats_df <- summary_stats_df %>%
    dplyr::select(parameter, names(.)[1:4])

# Create a nicely formatted table using kable and kableExtra
summary_table <- summary_stats_df %>%
    knitr::kable("html",
        caption = "Summary Statistics for Thinned Samples for Question 1a)",
        col.names = c("Parameter", "Estimate", "SD", "2.5%", "97.5%")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
print(summary_table)

# diagnostics!
geweke_results <- coda::geweke.diag(thinned_samples)
ess_results <- coda::effectiveSize(thinned_samples)

diags_df <- data.frame(
    parameter = c("beta1", "beta2"),
    ESS = unname(ess_results),
    geweke = unname(geweke_results$z)
)
diags_table <- diags_df %>%
    knitr::kable("html",
        caption = "Convergence Diagnostics for Thinned Samples for Question 1a)",
        col.names = c("Parameter", "Effective Sample Size", "Geweke Diagnostic")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
```
```{r}
# trace plots for thinned samples
par(mfrow = c(ceiling(p / 2), 2))
for (j in 1:p) {
    plot(thinned_samples[, j], type = "l", main = paste("Trace plot for β", j, "(thinned)"), ylab = expression(beta[j]), xlab = "Iteration (thinned)")
}

# Autocorrelation plots for thinned samples
par(mfrow = c(ceiling(p / 2), 2))
for (j in 1:p) {
    acf(thinned_samples[, j], main = paste("Autocorrelation for β", j, "(thinned)"))
}
```

```{r}
library(pgdraw)
library(mvtnorm)
n <- length(y)
p <- ncol(x)

n_iter <- 100000 # num of iterations
burn_in <- 2000 # burn in
thin_interval <- 10 # thinning interval

# init the values and objects to store things in
omega <- rep(1, n)
beta_samples <- matrix(0, nrow = n_iter, ncol = p)

for (t in 1:n_iter) {
    # Step 1: Sample omega given the current beta
    for (i in 1:n) {
        # Calculate the linear predictor for the i-th observation
        psi_i <- x[i, ] %*% beta

        # Sample omega_i | beta using Polya-Gamma(1, psi_i)
        omega[i] <- pgdraw(1, psi_i)
    }

    # Step 2: Sample beta given omega and y
    # Calculate posterior covariance (Sigma_beta) and mean (mu_beta)
    Omega <- diag(omega)
    Sigma_beta <- solve(t(x) %*% Omega %*% x + diag(1 / 10000, p))
    mu_beta <- Sigma_beta %*% t(x) %*% (y - 0.5)

    # Draw beta from the conditional multivariate normal distribution
    beta <- as.vector(rmvnorm(1, mean = mu_beta, sigma = Sigma_beta))

    # Store the sample
    beta_samples[t, ] <- beta
}

# discard burnin
post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]

# thin them samples
thin_interval <- 10
thinned_samples <- post_burn_samples[seq(1, nrow(post_burn_samples), thin_interval), ]
summary_stats_thinned <- apply(thinned_samples, 2, function(x) {
    c(Mean = mean(x), SD = sd(x), `2.5%` = quantile(x, 0.025), `97.5%` = quantile(x, 0.975))
})

summary_stats_df <- as.data.frame(t(summary_stats_thinned))
summary_stats_df$parameter <- c("beta1", "beta2")
summary_stats_df <- summary_stats_df %>%
    dplyr::select(parameter, names(.)[1:4])

# nice table
summary_table <- summary_stats_df %>%
    knitr::kable("html",
        caption = "Summary Statistics for Thinned Samples for Question 1b-iii)",
        col.names = c("Parameter", "Estimate", "SD", "2.5%", "97.5%")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )
print(summary_table)

# diagnostics!
geweke_results <- coda::geweke.diag(thinned_samples)
ess_results <- coda::effectiveSize(thinned_samples)

diags_df <- data.frame(
    parameter = c("beta1", "beta2"),
    ESS = unname(ess_results),
    geweke = unname(geweke_results$z)
)
diags_table <- diags_df %>%
    knitr::kable("html",
        caption = "Convergence Diagnostics for Thinned Samples for
        Question 11b-iii)",
        col.names = c("Parameter", "Effective Sample Size", "Geweke Diagnostic")
    ) %>%
    kableExtra::kable_styling(
        full_width = FALSE,
        position = "left", bootstrap_options = c("striped", "hover", "condensed")
    )

# Trace plots
par(mfrow = c(ceiling(p / 2), 2))
for (j in 1:p) {
    plot(thinned_samples[, j], type = "l", main = paste("Trace plot for β", j), ylab = expression(beta[j]), xlab = "Iteration (thinned)")
}

# Autocorrelation plots
par(mfrow = c(ceiling(p / 2), 2))
for (j in 1:p) {
    acf(thinned_samples[, j], main = paste("Autocorrelation for β", j, "(thinned)"))
}
```

```{r}
# Function for Metropolis Sampling (Part A)
metropolis_sampling <- function(y, x, n_iter, burn_in, thin_interval) {
    n <- length(y)
    p <- ncol(x)

    # Function to calculate the log posterior
    log_posterior <- function(beta, y, x) {
        eta <- x %*% beta
        log_lik <- sum(y * eta - log(1 + exp(eta)))
        log_prior <- -0.5 * t(beta) %*% beta / 10000
        return(log_lik + log_prior)
    }

    beta_init <- rep(0, p)
    proposal_sd <- 0.2
    beta_samples <- matrix(0, nrow = n_iter, ncol = p)
    beta_samples[1, ] <- beta_init

    set.seed(123)
    accept_count <- 0 # Initialize acceptance counter

    for (i in 2:n_iter) {
        current_beta <- beta_samples[i - 1, ]

        # Propose a new beta
        proposed_beta <- current_beta + rnorm(p, mean = 0, sd = proposal_sd)

        # Calculate log-posterior for current and proposed values
        log_posterior_current <- log_posterior(current_beta, y, x)
        log_posterior_proposed <- log_posterior(proposed_beta, y, x)

        # Acceptance probability
        accept_prob <- exp(log_posterior_proposed - log_posterior_current)

        # Accept or reject
        if (runif(1) < accept_prob) {
            beta_samples[i, ] <- proposed_beta
            accept_count <- accept_count + 1 # Increment if accepted
        } else {
            beta_samples[i, ] <- current_beta
        }
    }

    # Remove burn-in and thin samples
    post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]
    thinned_samples <- post_burn_samples[seq(1, nrow(post_burn_samples), thin_interval), ]

    return(thinned_samples)
}

# Function for Gibbs Sampling (Part B)
gibbs_sampling <- function(y, x, n_iter, burn_in) {
    n <- length(y)
    p <- ncol(x)

    beta <- rep(0, p)
    omega <- rep(1, n)
    beta_samples <- matrix(0, nrow = n_iter, ncol = p)

    for (t in 1:n_iter) {
        for (i in 1:n) {
            psi_i <- x[i, ] %*% beta
            omega[i] <- pgdraw(1, psi_i)
        }

        Omega <- diag(omega)
        Sigma_beta <- solve(t(x) %*% Omega %*% x + diag(1 / 10000, p))
        mu_beta <- Sigma_beta %*% t(x) %*% (y - 0.5)

        beta <- as.vector(rmvnorm(1, mean = mu_beta, sigma = Sigma_beta))

        beta_samples[t, ] <- beta
    }

    post_burn_samples <- beta_samples[(burn_in + 1):n_iter, ]

    return(post_burn_samples)
}

# Comparison Function
compare_results <- function(y, x) {
    n_iter <- 100000
    burn_in <- 2000
    thin_interval <- 10

    # Run and time Metropolis Sampling
    time_metropolis <- system.time({
        beta_metropolis <- metropolis_sampling(y, x, n_iter, burn_in, thin_interval)
    })

    # Run and time Gibbs Sampling
    time_gibbs <- system.time({
        beta_gibbs <- gibbs_sampling(y, x, n_iter, burn_in)
    })

    # Calculate diagnostics for Metropolis
    mcmc_metropolis <- mcmc(beta_metropolis)
    geweke_metropolis <- geweke.diag(mcmc_metropolis)
    ess_metropolis <- effectiveSize(mcmc_metropolis)
    autocorr_metropolis <- autocorr(mcmc_metropolis)

    # Calculate diagnostics for Gibbs
    mcmc_gibbs <- mcmc(beta_gibbs)
    geweke_gibbs <- geweke.diag(mcmc_gibbs)
    ess_gibbs <- effectiveSize(mcmc_gibbs)
    autocorr_gibbs <- autocorr(mcmc_gibbs)

    # Create results summary
    results <- list(
        time_metropolis = time_metropolis,
        time_gibbs = time_gibbs,
        geweke_metropolis = geweke_metropolis,
        geweke_gibbs = geweke_gibbs,
        ess_metropolis = ess_metropolis,
        ess_gibbs = ess_gibbs,
        autocorr_metropolis = autocorr_metropolis,
        autocorr_gibbs = autocorr_gibbs
    )

    return(results)
}

# Run the comparison
comparison_results <- compare_results(y, x)

# Print the results
print(comparison_results)
```